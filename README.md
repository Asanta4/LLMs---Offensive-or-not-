# LLMs---Offensive-or-not-
Toxic &amp; Harmful Text Classifier using Traditional NN, Mistral, and Llama LLMs
